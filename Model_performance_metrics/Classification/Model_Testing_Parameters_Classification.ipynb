{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6bb7ccc5-3720-4200-837f-c8875a1467a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing just numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5eb44b35-6bc2-486f-9918-57eaf6e3a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dataset using numpy\n",
    "data = np.loadtxt('Data/Random_classification.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404820f8-bbc5-4945-8ab3-3a4a15ac590f",
   "metadata": {},
   "source": [
    "**Mertics Explored:**\n",
    "- TP\n",
    "- TN\n",
    "- FP\n",
    "- FN\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1_Score\n",
    "- Specificity\n",
    "- Confusion Matrix\n",
    "- False Positive Ratio\n",
    "- Balanced Accuracy\n",
    "- Matthews_Correlation_Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832b2a8-7dd5-468b-aa06-6b7ae716ea3b",
   "metadata": {},
   "source": [
    "# Some important Definitions for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "812cb9c8-983d-4839-b449-a48845bed714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive\n",
    "def True_positive(data):\n",
    "    \"\"\" Number of instances which are correctly marked as 1 \"\"\"\n",
    "    true_positive=0\n",
    "    for i in range(len(data)):\n",
    "        if (data[i][0]== 1) & (data[i][0]== data[i][1]):\n",
    "            true_positive= true_positive + 1\n",
    "    return true_positive\n",
    "\n",
    "# True Negative\n",
    "def True_Negative(data):\n",
    "    \"\"\" Number of instances which are correctly marked as 0   \"\"\"\n",
    "    true_negative=0\n",
    "    for i in range(len(data)):\n",
    "        if (data[i][0]== 0) & (data[i][0]== data[i][1]):\n",
    "            true_negative= true_negative + 1\n",
    "    return true_negative\n",
    "\n",
    "# False Positive\n",
    "def False_positive(data):\n",
    "    \"\"\" Number of instances which are wrongly marked as 0 \"\"\"\n",
    "    false_positive=0\n",
    "    for i in range(len(data)):\n",
    "        if (data[i][0]== 1) & (data[i][0]!= data[i][1]):\n",
    "            false_positive= false_positive + 1\n",
    "    return false_positive\n",
    "\n",
    "# True Negative\n",
    "def False_Negative(data):\n",
    "    \"\"\" Number of instances which are wrongly marked as 1  \"\"\"\n",
    "    false_negative=0\n",
    "    for i in range(len(data)):\n",
    "        if (data[i][0]== 0) & (data[i][0]!= data[i][1]):\n",
    "            false_negative= false_negative + 1\n",
    "    return false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "118c1fce-3b30-4a05-9d16-95b4b6e67574",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = True_positive(data)\n",
    "TN = True_Negative(data)\n",
    "FP = False_positive(data)\n",
    "FN = False_Negative(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd30a4d-d4a3-4cfe-a7eb-5ae7ca305130",
   "metadata": {},
   "source": [
    "# Checking the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94279ced-ec6b-45a1-88e7-776a5de1527f",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14634343-9870-4f2b-a77a-220c816f2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy \n",
    "def Accuracy(TP,FP,TN,FN):\n",
    "    \"\"\" Fornuma= (TP+TN/TP+FP+TN+FN)\n",
    "    Range: [0,1]\n",
    "    Measures the number of currect prediction over the total prediction\n",
    "    Best use: Best used when classes are balanced and the cost of false positives and false negatives is similar\n",
    "    Not to use: In imbalanced datasets, a high accuracy might simply reflect the majority class and mask poor performance on the minority class.\n",
    "    )\"\"\"\n",
    "    accuracy= (TP+TN)*100/(TP+FP+TN+FN)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb216401-adf1-47db-8113-9c7dfa0eac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "def Precision(TP,FP):\n",
    "    \"\"\" Precison: TP/TP+FP \n",
    "    Range: [0,1]\n",
    "    Percentage of true cases which are predicted as True\n",
    "    Best use: Important when the cost of false positives is high (e.g., spam detection, medical diagnosis where a false alarm could lead to unnecessary treatments)\n",
    "    Not to use: High precision does not account for false negatives; a model might only be predicting a very few positives to keep FP low\"\"\"\n",
    "    precision= TP/(TP+FP)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a902b323-17e4-4f38-aab1-2856e1f01ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "def Recall(TP, FN):\n",
    "    \"\"\"Recall: TP/TP+FN also known as True_Positive_Ratio or Sensitivity\n",
    "    Range: [0,1]\n",
    "    Precentage of true prediction which are actually true\n",
    "    Best use: Critical when the cost of missing a positive instance is high (e.g., disease screening where missing a true case is dangerous)\n",
    "    Not to use: Focusing solely on recall may increase the number of false positives, which is not ideal in scenarios where false alarms are problematic\"\"\"\n",
    "    recall = TP/(TP+FN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea3cf73f-6fb4-4047-9506-50f05efe8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1_score\n",
    "def F1_score(precision= Precision(TP,FP), recall= Recall(TP, FN)):\n",
    "    \"\"\"F1 score: 2*(Precision * Recall)/(Precision + Recall)\n",
    "    Range: [0,1]\n",
    "    Best use: Useful when you need a balance between precision and recall, particularly in cases of uneven class distribution\n",
    "    Not to use: Does not take true negatives into account, so it might not reflect overall model performance in certain contexts\"\"\"\n",
    "    f1_score= 2* (precision * recall)/ (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f2ae4e5-702e-4207-adab-42c6f2e0b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificity\n",
    "def Specificity(TN, FP):\n",
    "    \"\"\" \n",
    "    Specificity = TN/(TN+FP)\n",
    "    Precentage of false prediction correctly predicted as false\n",
    "    Best use: Valuable when the correct identification of negatives is important (e.g., confirming that a healthy patient is truly healthy)\n",
    "    Not to use: In datasets with a high number of negatives, specificity might appear high even if the model misses many positives\n",
    "    \"\"\"\n",
    "    specificity= TN/(TN+FP)\n",
    "    return specificity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dba5fc78-8bd4-44d7-9069-9a93920427d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion_matrix\n",
    "def Confusion_matrix(TP, FP, TN, FN):\n",
    "    \"\"\" \n",
    "    Confusion matrix [TP, FN],\n",
    "                     [FP, TN] \n",
    "    Best use: Useful when you need a balance between precision and recall, particularly in cases of uneven class distribution\n",
    "    Not to use: Does not take true negatives into account, so it might not reflect overall model performance in certain contexts\n",
    "    \"\"\"\n",
    "    return np.array([[TP, FN],\n",
    "                     [FP, TN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0ab183c-53ac-44c6-a0bd-f8676650d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positive Ratio\n",
    "def False_Positive_Ratio(FP, TN):\n",
    "    \"\"\"\n",
    "    False_Positive_Ratio: FP/ (FP+TN)\n",
    "    \"\"\"\n",
    "    false_positive_ratio= FP/ (FP+TN)\n",
    "    return false_positive_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74de4a22-038d-41fe-a2e0-7d4bf2018812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve and AUC (Area Under the Curve)\n",
    "import matplotlib.pyplot as plt\n",
    "def ROC_curve():\n",
    "    \"\"\"\n",
    "    The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings\n",
    "    \"\"\"\n",
    "    # Get the unique thresholds from the predicted scores in descending order.\n",
    "    thresholds = np.sort(np.unique(ata[:, 1]))[::-1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bf3bc3d-8fd7-4b7e-b98d-3062606160b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Accuracy\n",
    "def Balanced_accuracy(sensitivity = Recall(TP, FN), specificity= Specificity(TN, FP)):\n",
    "    \"\"\"\n",
    "    Balanced Accuracy: (Sensitivity+Specificity)/2\n",
    "    Best use: Helps when dealing with imbalanced datasets, ensuring both classes are given equal weight\n",
    "    Not to use: While it provides a better balance than plain accuracy, it can still mask poor performance on one of the classes if the other is extremely high\n",
    "    \"\"\"\n",
    "    balance_accuracy= (sensitivity+specificity)/2\n",
    "    return balance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1acd6474-8a14-4691-88d3-5a51913c5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matthews Correlation Coefficient (MCC)\n",
    "def Matthews_Correlation_Coefficient(TP,TN,FP,FN):\n",
    "    \"\"\"\n",
    "    MCC= TP×TN−FP×FN/ sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))\n",
    "    Best use: Considered one of the best measures for binary classification, especially on imbalanced datasets, as it takes all four quadrants of the confusion matrix into account\n",
    "    Not to use: The metric can be harder to interpret intuitively compared to accuracy, precision, or recall\n",
    "    \"\"\"\n",
    "    MCC= ((TP*TN)-(FP*FN))/ np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "\n",
    "    return MCC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
